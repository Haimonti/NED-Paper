\section{Related Work}
Name entity disambiguation is popular area of research. But when it comes to the NED in noisy text, definition of noisy data is limited to the free text unstructured data. Most of the studies use Wikipedia as their knowledge base. ~\cite{cucerzan2007large} realized the effectiveness of using topical coherence to help named entity disambiguation. They used overlaps in categories and incoming links in Wikipedia to calculate the topical coherence between the referent entity candidate and other entities within the same context. Furthermore \cite{kataria2011entit} proposed the weakly semi-supervised hierarchical topic model called Wikipedia-based Pachinko Allocation Model (WPAM ) for disambiguating entities. WPAM uses all the words in a document, including those in the vicinity of un-annotated references, to learn high-quality word-entity associations They used Wikipedia annotations to appropriately bias the assignment of entity labels to annotated words (and un-annotated words co-occurring with them), and the Wikipedia category hierarchy to capture entity context and co-occurrence patterns in a single unified disambiguation framework. \cite{sen2012collective} adopted a latent topic model to learn the context-entity association to help disambiguation.
\cite{bhattacharya2006latent} developed a probabilistic generative model for collectively resolving entities in relational data. Their model may be viewed as extending the Dirichlet process mixture model to capture relations between entities or components. They modeled the topical coherence as the association of an entity and the latent topics of a document \cite{pilz2011names} approached the problem of name ambiguity using thematic distances over describing documents. Their approach relies on semantic topics provided by LDA and is thus able to exploit more information than other, rather restrictive, word-matching. Methods. \cite{glaser2016named} proposed NED to disambiguate unknown persons without any specific textual training data. Instead, it uses some aggregate of the textual material about different people that share the same properties (e.g., nationalities and professions). Then they applied topic modeling on the data to obtain topic information. To disambiguate a new unknown person in a text document, they obtained topic information from the context of the person, then compared this topic information with the information from the extracted material to find out which properties are closest to the person. \cite{Li:2013} proposed a generative model and an incremental algorithm to automatically mine useful evidences across documents. They used topic modeling for getting background topic and unknown entities.
